<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Probador Virtual de Gorras AR Simplificado</title>
    <style>
        body { margin: 0; overflow: hidden; font-family: Arial, sans-serif; }
        #loading-screen {
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background-color: rgba(0, 0, 0, 0.8);
            display: flex;
            justify-content: center;
            align-items: center;
            z-index: 1000;
            color: white;
            font-size: 24px;
        }
        #error-message {
            position: fixed;
            top: 50%;
            left: 50%;
            transform: translate(-50%, -50%);
            background-color: rgba(255, 0, 0, 0.8);
            color: white;
            padding: 20px;
            border-radius: 10px;
            text-align: center;
            display: none;
        }
    </style>
</head>
<body>
    <div id="loading-screen">Cargando experiencia AR...</div>
    <div id="error-message"></div>

    <!-- Importación de bibliotecas -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/three.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-core"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-converter"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/face-landmarks-detection"></script>

    <script>
    let scene, camera, renderer, hat;

    async function init() {
        try {
            // Configuración de Three.js
            scene = new THREE.Scene();
            camera = new THREE.PerspectiveCamera(75, window.innerWidth / window.innerHeight, 0.1, 1000);
            renderer = new THREE.WebGLRenderer({ antialias: true, alpha: true });
            renderer.setSize(window.innerWidth, window.innerHeight);
            document.body.appendChild(renderer.domElement);

            // Iluminación
            const ambientLight = new THREE.AmbientLight(0xffffff, 0.5);
            scene.add(ambientLight);
            const directionalLight = new THREE.DirectionalLight(0xffffff, 0.8);
            directionalLight.position.set(0, 1, 0);
            scene.add(directionalLight);

            // Cargar modelo de gorra
            await loadHatModel('https://nicolascumbej.github.io/arexperience/tmp2h0xvw1a.obj');

            // Configurar detección facial
            await setupFaceDetection();

            // Iniciar animación
            animate();

            document.getElementById('loading-screen').style.display = 'none';
        } catch (error) {
            console.error('Error durante la inicialización:', error);
            showError('Hubo un error al iniciar la experiencia AR. Por favor, recarga la página.');
        }
    }

    async function loadHatModel(url) {
        return new Promise((resolve, reject) => {
            const loader = new THREE.OBJLoader();
            loader.load(url, 
                (object) => {
                    hat = object;
                    scene.add(hat);
                    hat.scale.set(0.1, 0.1, 0.1);
                    resolve();
                },
                undefined,
                (error) => {
                    console.error('Error cargando el modelo:', error);
                    reject(error);
                }
            );
        });
    }

    async function setupFaceDetection() {
        try {
            const model = await faceLandmarksDetection.load(
                faceLandmarksDetection.SupportedPackages.mediapipeFacemesh);
            
            const video = document.createElement('video');
            video.srcObject = await navigator.mediaDevices.getUserMedia({ video: true });
            await video.play();

            async function detectFace() {
                const faces = await model.estimateFaces({ input: video });
                if (faces.length > 0) {
                    const face = faces[0];
                    updateHatPosition(face);
                }
                requestAnimationFrame(detectFace);
            }

            detectFace();
        } catch (error) {
            console.error('Error en la detección facial:', error);
            showError('No se pudo acceder a la cámara. Asegúrate de que tienes una cámara conectada y has dado permiso para usarla.');
        }
    }

    function updateHatPosition(face) {
        const nose = face.annotations.noseTip[0];
        hat.position.set(nose[0], nose[1] + 0.1, -nose[2]);
        hat.rotation.set(-face.rotation.x, -face.rotation.y, -face.rotation.z);
    }

    function animate() {
        requestAnimationFrame(animate);
        renderer.render(scene, camera);
    }

    function showError(message) {
        const errorElement = document.getElementById('error-message');
        errorElement.textContent = message;
        errorElement.style.display = 'block';
        document.getElementById('loading-screen').style.display = 'none';
    }

    window.addEventListener('resize', () => {
        if (camera && renderer) {
            camera.aspect = window.innerWidth / window.innerHeight;
            camera.updateProjectionMatrix();
            renderer.setSize(window.innerWidth, window.innerHeight);
        }
    });

    // Iniciar la aplicación
    init();
    </script>
</body>
</html>
